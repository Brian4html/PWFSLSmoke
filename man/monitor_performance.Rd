% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/monitor_performance.R
\name{monitor_performance}
\alias{monitor_performance}
\title{Calculate Monitor Prediction Performance}
\usage{
monitor_performance(predicted, observed, t1, t2, metric = NULL, FPCost = 1,
  FNCost = 1)
}
\arguments{
\item{predicted}{data list of class \code{ws_monitor}}

\item{observed}{data list of class \code{ws_monitor}}

\item{t1}{value used to classify \code{predicted} measurements}

\item{t2}{threshold used to classify \code{observed} measurements}

\item{metric}{confusion matrix metric to be used}

\item{FPCost}{cost associated with false positives (type I error)}

\item{FNCost}{cost associated with false negatives (type II error)}
}
\value{
Dataframe of monitors vs named measure of performance.
}
\description{
This function uses "confusion matrix" analysis to calculate
different measures of predictive performance for every timeseries found
in \code{predicted} with respect to the observed values found in the single timeseries
found in \code{observed}.

The requested metric is returned in a dataframe organized with one row per monitor,
all available metrics are returned.
}
\seealso{
\link{monitor_performanceMap}

\link{skill_confusionMatrix}
}
\keyword{monitor}

