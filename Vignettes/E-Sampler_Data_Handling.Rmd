---
title: "AIRSIS Data Acquisition and Handling"
author: "Rex Thompson - Mazama Science"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(PWFSLSmoke)
logger.setup()
```

This document covers the basics of how the Mazama Science PWFSL Package acquires and processes raw AIRSIS data (from EBAM and ESAM monitors) to create a raw data dataframe. Relevant functions include those called by the `airsis_createRawDataFrame` function:

* airsis_downloadData
* airsis_parseData
* airsis_qualityControl
* addClustering

## Downloading Data

The first step in `airsis_createRawDataFrame` function is to download the raw data for the requested monitor and time period. This is done using the `airsis_downloadData` function. The user passes in an agency (e.g. 'USFS') and a monitor ID (e.g. '1033') and a time period.  Data is then downloaded as a raw text string from the relevant agency's AIRSIS website (e.g. <http://usfs.airsis.com>).

For example, the following code will create text strings of raw data for two different monitors over the month of September 2016.
```{r}
fileStringEBAM <- airsis_downloadData('USFS', unitID='1012', startdate=20160901, enddate=20160930)
fileStringESAM <- airsis_downloadData('USFS', unitID='1050', startdate=20160901, enddate=20160930)
```

## Parsing Data

Once a raw data text string is in the system it is parsed and preliminarily cleaned by the `airsis_parseData` function. Note that this function currently only supports AIRSIS EBAM and ESAM monitors. Functionality may be added in the future to parse and clean BAM1020 monitor data, as well as deprecated monitor types (e.g. EBAM 1, EBAM 2). The parsing process returns a semi-clean raw data dataframe for the monitor of interest (additional data cleaning occurs later -- see the Quality Control section below). The following summarizes the preliminary cleaning that is performed on the data during the parsing process:

* E-Sampler: It has been observed that some E-Sampler files from AIRSIS (USFS 1050) have internal rows messed up with header line information.  The parse process removes rows such as these, which can be identified by searching for '%'.
* E-Sampler: Data for UnitID=1050 in July 2016 was observed to have extra rows with some sort of metadata in columns 'Serial.Number' and 'Data.1'. We remove such rows.
* All types: Latitude, Longitude and Sys..Volts are measured at 6am and 6pm as separate GPS entries in the dataframe. These records are carried forward so they appear in all rows.

Continuing from our example above, the following code creates a dataframe for each of the two text strings created above:
```{r}
dfEBAM <- airsis_parseData(fileStringEBAM)
dfESAM <- airsis_parseData(fileStringESAM)
```

## Quality Control (QC)

Once the raw data is parsed and polished a bit, it is passed into the `airsis_qualityControl` funtion, which is where the majority of the automated data cleaning takes place. A few preliminary checks are performed before the dataframe is passed into a monitor type-specific QC function. Currently, two such functions exist (i.e. `airsis_EBAMQualityControl`, for EBAM monitors, and `airsis_EBAMQualityControl`, for ESAM monitors). More QC functions may be added at a later date for additional monitor types.

Although separate, the functionality between the two current QC functions is nearly identical, and so they will be addressed together within the sections below. The following sub-sections provide an overview of the different elements of the QC process.

### Longitudes/Latitudes

The QC functions check monitor coordinate values against longitude and latitude limits. Default limits are as follows:

* Longitude: -180 to 180
* Latitude: -90 to 90

Any rows with coordinate values outside the limits are removed from the QC'd dataframe.

### Assigning Time

An important step in the quality control process is assigning consistent time stamps to the data so the plotting and other functions in the PWFSLSmoke package can work consistently on all data.

The EBAM and ESAM raw data includes various date/time fields. Below is a summary of the various fields, as well as our current understanding as to what each represents:

* `Date.Time.GMT` (EBAM only) - date/time of the data
* `Start.Date.Time..GMT.` (almost always data blank for both monitor types; only observed once thus far, on USFS Monitor 1012 on 9/30/16, possibly due to a monitor software version update)
* `TimeStamp` - date/time the data was received by AIRSIS
* `PDate` - date/time the data was processed by AIRSIS

For the EBAM monitors, the `Date.Time.GMT` data is always at the top of an hour, and we more or less take it at face value to represent the time associated with the data (in actuality we use the floor of the `TimeStamp` data, though maybe we should revisit the appropriateness of this decision to protect against assigning data to the wrong time in case the data ever comes in more than an hour after the time for which it is representative). However, this nice clean `Date.Time.GMT` field is not provided for the ESAM monitors. For the ESAM monitors, we use the floor of the `TimeStamp` data to assign a clean `datetime` field.

While the EBAM monitors' `TimeStamp` data is typically just a few minutes after the top of an hour, this is not the case with the ESAM monitors. The ESAM monitors' `TimeStamp` data is observed to drift by a few minutes more than 60 between readings (60 is the inteval we would excpet between hourly readings), except for once a day when the readings appear to "reset" to being captured just a few minutes past the top of the hour. For example, the following plot shows the number of minutes __*after*__ the top of the hour of each `TimeStamp` row for USFS monitor ID 1049 (ESAM) in September 2016.

```{r, echo=FALSE}
tsESAM <- lubridate::mdy_hms(dfESAM$TimeStamp)
minsESAM <- lubridate::minute(tsESAM)
plot(tsESAM,minsESAM,xlab="TimeStamp",ylab="TimeStamp Minute",ylim=c(0,25))
title("Minutes Past the Top of the Hour\nUSFS Monitor ID 1050 TimeStamp Data\nSeptember 2016")
```

Similar patterns were observed on other ESAM monitors as well. In some cases, the time offset was observed to become quite large (up to 55 minutes) before resetting; for example, see USFS Monitor ID 1049 data in September 2016. But, importantly, it appears the data offset never gets so large as to carry over to the next full hour (though it might not hurt to periodically check for this occurrence). As such, we are able to simply take the floor of the `TimeStamp` data in order to assign the data to unique hours. This is the approach we have implemented in the `airsis_qualityControl` function to create a new `datetime` field in the QC'd dataframe.

### Data Type (EBAM Monitors Only)

The `airsis_EBAMQualityControl` function includes a check to ensure that the `Type` field is 'PM2.5' for all rows. Nonconforming rows are removed from the QC'd dataframe.

### Missing Data and Value Checks

The QC functions screen the data for missing values and check the existing values against acceptable limits. The following table summarizes the parameters that are checked, along with default limit thresholds. When calling the QC functions on their own, the user can specify alternate threshold values.

| Parameter                  | EBAM Name  | EBAM Limits      | ESAM Name     | ESAM Limits |
|:-------------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Sample Flow Rate           | `Flow`     | > +/- 5% of 16.7 | `Flow.l.m.`   | Not = 2     |
| Air Temperature            | `AT`       | > 45             | `AT.C.`       | > 150       |
| Relative Humidity          | `RHi`      | > 45             | `RHi...`      | > 55        |
| Hourly PM2.5 Concentration | `ConcHr`   | > 0.984          | `Conc.mg.m3.` | > 984       |
| Time Stamp                 | `datetime` | > now            | `datetime`    | > now       |

### Duplicte Hours

The last step in the AIRSIS QC functions is to go through the data and remove any duplicate records. Occasionally, the data will include more than one record for the same time period. In many cases this is observed as two records with the same `TimeStamp` times, but different `PDate` times, which implies that the data was reprocessed (in reality, the duplicate check is based on the values in the `datetime` field. In such cases, we assume the data was reprocessed for a reason and therefore we choose to retain the entry with the latest processing time (currently we simply retain the last record as the data always appears to be in order; could rigorize by actually adding logic to take the duplicate with the latest processing time). All prior rows with duplicate `TimeStamp` values are removed from the QC'd dataframe.

## Clustering

Finally, once the data is downloaded and cleaned, it is processed to assign an ID to unique deployments based on the latitude/longitude of the monitor over the period in question. The user passes in a ....

TO FURTHER DEVELOP THIS SECTION...

`addClustering`